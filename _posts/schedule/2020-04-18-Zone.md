---
layout: post
title:  "Multi-Region Patterns in CockroachDB"
date:   2020-04-18 17:06:57 +0800
categories: schedule 
---

## 简介

CockroachDB（以下简称 CRDB）是一款全球部署的分布式数据库，因此相当重视 Multi-Region 的场景，并针对这种场景实现了一系列的技术方案，用于避免 Multi-Region 带来的延时。

CRDB 的读写流程中使用 raft 协议，典型的 Multi-Region 集群拓扑是这样的：

* 为保证高可用，同一个 range 的副本放到不同的 region；即使有多个副本在同一个 region，也尽量放到不同的 zone
* Gateway node 读数据需要路由到 leaseholder，而 leaseholder 与 gateway node 可能在不同的 region，所以需要跨 region 访问
* Gateway node 写数据先路由到 leaseholder，leaseholder 写完后把 raft log 转发给 follower，多数副本确认后才返回。而 leaseholder 与 gateway node 可能在不同的 region，leaseholder 与 follower 一般也在不同的 region，所以需要多次跨 region 访问

以上这种拓扑，必然带来不少访问延时。下面就来介绍 Multi-Region 的一些方案。每个方案都有一些限制和优缺点，分别适用于不同的场景。

## Geo-Partitioning

当数据与请求的区域强关联时，CRDB 首推 Geo-Partitioning。

先看一个例子，假如有一张用户表：

```sql
CREATE TABLE users (
    id INT NOT NULL PRIMARY KEY,
    city STRING NOT NULL,
    name STRING NOT NULL
);
```

集群有多个数据中心，分布在各个城市。业务访问用户表时，通常只查询当地的用户，例如北京的 gateway node 更多地查询北京的用户。这就是数据与请求的区域强关联。

由于数据随机分布到各个数据中心，通常需要跨数据中心访问，延时较高。Geo-Partitioning 就是把表数据按区域分为多个分区，单独指定每个分区的存放位置，使数据更靠近 gateway node。

Geo-Partitioning 又可以细分为两个方案：Geo-Partitioned Replicas 和 Geo-Partitioned Leaseholders。在介绍这两个方案之前，先简单介绍下 CRDB 的 Replication Zones 和分区。

### Replication Zones

Geo-Partitioning 和后面要讲的 Duplicate Indexes 都基于 Replication Zones 这个特性。

Replication Zones 的理念是让用户自己配置数据的存放位置，从而使 CRDB 能以更细的粒度控制数据与区域的关联关系。Replication Zones 提供了 SQL 接口，支持 database / table / index / partition / range 等各个粒度的控制。

配置 Replication Zones 的基本语法如下：

```sql
ALTER {DATABASE | TABLE | INDEX | PARTITION | RANGE} name
	CONFIGURE ZONE USING variable=value [, ...]
```

其中 `variable` 是配置项，包括：

* range_min_bytes：该 zone 上 range 的最小值，小于该值需要合并 range
* range_max_bytes：该 zone 上 range 的最大值，大于该值需要分裂 range
* gc.ttlseconds：失效数据的存活时间，超过该时间后会被回收
* num_replicas：副本数量，普通数据默认是 3，系统数据默认是 5
* constraints：副本的位置限制
* lease_preferences：leaseholder 的优先位置，越靠前优先级越高

其中 `constraints` 和 `lease_preferences` 是实现 Geo-Partitioning 和 Duplicate Indexes 的重要配置项。

### Partitions

CRDB 的分区和传统数据库的用途不一样，它的用途主要有：

* Geo-Partitioning，避免跨区域的访问
* Archival-Partitioning，冷热数据分离，把冷数据放在更便宜的硬件上，降低成本
* Data sovereignty，根据法规把特定数据放在特定区域，保证信息安全

可以看到，这些用途有一个共同目标，就是把数据人为地放到不同位置。基于以上用途，CRDB 支持了两种分区类型：Range 和 List。其中 Range 主要用于 Archival-Partitioning，List 用于 Geo-Partitioning 和 Data sovereignty。

为保证主键 / 唯一键全局唯一，分布式数据库一般要求主键 / 唯一键包含分区键。但 CRDB 的限制比较极端：所有索引键要以分区键为前缀。由于这条限制，CRDB 分区表的 key 的格式可以表示为：

* 主表：`/{table_id}/{index_id}/{primary_key}/…`
* 二级索引：`/{table_id}/{index_id}/{index_key}/{primary_key}/…`

可以看到，CRDB 不会单独地分配 `partition_id`，而是沿用 `table_id`，所以 key 的格式与普通表完全相同。这样带来一个好处：在新增 / 删除 / 变更分区时，不需要涉及任何数据迁移，而是只需要更新元数据。这里的元数据用来存储每个分区的范围。

因为 key 以分区键为前缀，所以对于 Range 分区，一个分区仅包含一个 key range；对于 List 分区，一个分区由多段 key range 组成，每个分区键对应一个 key range。所以元数据的数据量很小。

在 Geo-Partitioning 中，分区键一定要与区域的划分相关。在上面 `users` 表的例子中， `city` 列与区域相关，所以 `city` 可以作为分区键，例如：

```sql
ALTER TABLE users PARTITION BY LIST (city) (
    PARTITION bj VALUES IN ('beijing'),
    PARTITION sh VALUES IN ('shanghai')
);
```

假如表中找不到像 `city` 这样合适的字段，也可以用 computed column（MySQL 中叫 generated column）作为分区键。

二级索引默认没有分区，但是 Geo-Partitioning 要求二级索引也必须分区，且分区键要与主表相同。

综上，CRDB 把分区做得非常轻量级，基本上分区就是用来框定数据的范围。同时，分区也有很多的限制。

### Geo-Partitioned Replicas

Geo-Partitioned Replicas 就是把单个分区的**所有 replica** 全部放到同一个区域。

这不可避免地会牺牲可用性，因为一旦整个区域的节点全都不可用，整个集群也不可用了。但区域会细分为多个 zone，CRDB 的调度器会把副本尽量分散到不同 zone 上，以最大地保证可用性。

这种方案带来的好处是，读和写都不需要跨区域访问了，因为所有数据都在同一个区域。

定义 Geo-Partitioned Replicas 的例子：

```sql
ALTER PARTITION bj OF INDEX users@*
    CONFIGURE ZONE USING constraints = '[+region=bj]';
ALTER PARTITION sh OF INDEX users@*
    CONFIGURE ZONE USING constraints = '[+region=sh]';
```

这里把分区 `bj` 的主表和所有索引都放到 `bj` 区域，把分区 `sh` 放到 `sh` 区域。

### Geo-Partitioned Leaseholders

Geo-Partitioned Leaseholders 只把分区的 **leaseholder** 放到同一个区域，而 follower 会由 CRDB 调度到其他区域。

这样保证了可用性，读延时也很小。但是写数据要写其他区域上的副本，所以写延时仍然很高。

定义 Geo-Partitioned Leaseholders 的例子：

```sql
ALTER PARTITION bj OF INDEX users@*
    CONFIGURE ZONE USING
      num_replicas = 3,
      constraints = '{"+region=bj":1}',
      lease_preferences = '[[+region=bj]]';
```

分区 `bj` 仍然总共有 3 个副本，其中优先把 leaseholer 放到 `bj` 区域，且 `bj` 只有一个副本。

## Duplicate Indexes

Geo-Partitioning 适用于数据与请求的区域强关联的场景，对于其他场景，CRDB 提供了 Duplicate Indexes 方案。

该方案类似于有些分布式数据库中的广播表（有时叫复制表）：对于一张频繁访问的小表，每个节点都存放一份全量的数据，既能减少跨节点的访问，又能把负载均摊。而 Duplicate Indexes 有些不一样，它让每个区域存放一个 leaseholder，以减少跨区域的访问。

Duplicate Indexes 的实现很巧妙：建一些覆盖索引，并且索引键与主键一样，那么这样的二级索引就与主表没有什么差异了；再通过 Replication Zones 把各个索引的 leaseholder 分别放到不同的区域，那么每个区域可以只访问本区域的那个索引。这套方案实际上就是利用索引和 Replication Zones 实现了等同广播表的功能，免去了创造新语法。

以 `users` 表为例。假设只有 `bj` 和 `sh` 这 2 个数据中心，并且请求没有明显的区域属性，即北京的 gateway node 也经常访问上海的用户数据。可以建一个覆盖索引：

```sql
CREATE INDEX idx ON users (id)
    STORING (city, name);
```

这里，索引 `idx` 的索引键 `id` 就是主键；`STORING(city, name)` 指索引 `idx` 的数据中也带上 `city`, `name` 这两列，避免回表查询。这样，`idx` 的表结构实质上与 `users` 几乎一样了。

然后再设置 `users` 的 leaseholder 在 `bj`，`idx` 的 leaseholder 在 `sh`：

```sql
ALTER TABLE users
    CONFIGURE ZONE USING
      constraints = '{"+region=bj":1}',
      lease_preferences = '[[+region=bj]]';
ALTER INDEX users@idx
    CONFIGURE ZONE USING
      constraints = '{"+region=sh":1}',
      lease_preferences = '[[+region=sh]]';
```

处理读请求时，CRDB 会利用 Locality-Aware Index Selection 自动地选择最近的索引。普通的分布式数据库在选择索引时一般只考虑数据量，而 CRDB 还考虑了跨区域访问的延时。在相同的几个索引中，CRDB 会选择最近的那个。

这样，北京的 gateway node 只需要访问 `users` 表，上海的 gateway node 访问 `idx` 表，避免了跨区域访问。类似地，假如集群有 N 个区域，就要建 N - 1 个二级索引，然后分别配置 Replication Zones。

Duplicate Indexes 的缺陷也显而易见。首先是写放大非常大，N 个区域，意味着要写 N 个 range。以三副本为例，就要写 3 * N 份数据。这也不可避免地带来更高的写延时。其次，消耗的存储空间也更大了。

所以，Duplicate Indexes 适用于那些几乎不需要修改的表。

## Follower Reads

Duplicate Indexes 为了保证线性一致性，仍然只允许 leaseholder 读数据，导致创建了 3 * N 个副本。很多分布式数据库使用了读写分离方案，就是也允许 follower 读数据，不仅能利用局部性，还能减轻 leaseholder 的负载。

CRDB 的 Follower Reads 就是类似的方案，允许 follower 处理读请求，但只是为了避免跨区域的访问。Follower 上的数据并不保证最新（官方声称至少 48 秒，甚至会更早），所以是弱一致性读。这一般比较危险，所以需要用户在 SQL 中声明要读历史数据，并且允许在 follower 上读。例如：

```sql
SELECT name FROM users
    AS OF SYSTEM TIME experimental_follower_read_timestamp()
    WHERE id = 5;
```

如果对 `name` 的实时性要求不高的话，就可以加上 `AS OF SYSTEM TIME experimental_follower_read_timestamp()`，告诉 CRDB 可以读离 gateway node 更近的 follower 上的历史数据。

尽管是读历史数据，Follower Reads 还是提供了一个保障：读取之后，不会有提交时间更小的事务写入。

这是通过 closed timestamp（简称 CT，YugabyteDB 中也叫 safe time）实现的。每个 range 的 leaseholder 维护一个 CT。CRDB 保证不会有新的写入（例如 in-flight transaction），比 CT 还要早。如果还要早，CRDB 会拒绝该写入。节点之间会定期交换 CT，以保证 gateway node 能拿到每个 leaseholder 最新的 CT。

当 gateway node 收到请求后，判断请求满足以下条件时，才会尝试使用 follower read：

* SQL 包含 `AS OF SYSTEM TIME experimental_follower_read_timestamp()`
* 请求是读请求
* 请求的时间比 range 的 CT 早

Gateway node 决定 follower read 后，需要找一个最近的副本。每个节点都维护了一张表，记录它与其他节点之间的延时。通过心跳可以采样到最新的延时，然后使用 Exponentially Weighted Moving Average 算法来计算最近一段时间的平均延时。

选择副本的大体流程是：

* 如果 gateway node 上就有 range 的一个副本，直接在本地读
* 如果有节点之间的延时信息，则读最近平均延时最小的节点上的副本
* 如果没有延时信息（例如采样数量还不够），选择与 gateway node 的位置（例如 region / az / datacenter） 最匹配的节点。这个位置是节点启动时在 `--locality` 选项中指定的。

## Follow-the-Workload

Follow-the-Workload 可以说是最有意思的一个方案了。它不需要用户做什么操作，默认就自动运行。

想象这样一个场景：CRDB 集群部署在纽约和北京两个中心，全世界都会访问这个数据库。但访问量有一个特征，中国是白天的时候，中国的访问量更高；美国是白天的时候，美国的访问量更高。在同一时刻，都有一个最活跃的区域，所以把 leaseholder 固定在任一个区域都不是最佳的。

Follow-the-workload 就是用来解决这个问题的。CRDB 会监测每个数据中心的负载情况，并且把 leaseholder 自动地迁移到最活跃的区域，降低该区域的访问延时。所以 leaseholder 就像向日葵一样，跟随着太阳移动。由于迁移 leaseholder 不涉及数据的搬迁，所以也非常轻量级。

但是该方案放弃了其他情况的延时。不活跃的区域的读和写延时都很高；即使是活跃的区域，因为写数据也需要跨区域，所以延时仍然很高。但总体而言，性能得到了很大的提升。

## 总结

CRDB 的这些方案基本可以应对各类 Multi-Region 的场景，在业界做得非常领先。

几种方案的特征对比如下：

| 方案 | 限制 | 用户操作 | 写延时 | 可用性 | 一致性 |
| ---- | ---- | ---- | ---- | ---- | ---- |
| Geo-Partitioned Replicas | 数据与请求的区域强关联 | 分区；配置 Replication Zones | 降低 | 降低 |  |
| Geo-Partitioned Leaseholders | 数据与请求的区域强关联 | 分区；配置 Replication Zones |  |  |  |
| Duplicate Indexes |  | 建索引；配置 Replication Zones | 提高 | 提高 |  |
| Follower Reads |  | SQL 中加关键字 |  |  | 弱一致 |
| Follow-the-Workload |  |  |  |  |  |
